# -*- coding: utf-8 -*-
"""neurobot_reworked.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YjPg2kiijmFJe6rWmOzeNcn-mjePMSGt
"""

!pip install opencv-python pillow scikit-image

# Commented out IPython magic to ensure Python compatibility.
import cv2
from skimage import io, exposure, data
import numpy as np
from math import sqrt  # atan
import os

from pprint import pprint
from google.colab import drive
from google.colab.patches import cv2_imshow

drive.mount('/content/drive')

workpath = "/content/drive/MyDrive/nbt24/"
os.chdir(workpath),

# %matplotlib inline
from matplotlib import pyplot as plt

from tools.filters import bright_contrast, histogram_adp
from tools.processtool import get_dist_to_center, mask_by_color, mask_center, invert

def show(img, wd=300):
    h, w = img.shape[:2]
    ratio = w / h
    img = cv2.resize(img, (wd, round(wd / ratio)), interpolation = cv2.INTER_LINEAR)
    # img = cv2.cvtColor(img, cv2.COLOR_HSV2BGR)
    cv2_imshow(img)

image = cv2.imread(workpath + "1223.jpg")  # 1223.jpg 4438.jpg photo4.jpg rect4.png

def heq(iimg):
    # We first create a CLAHE model based on OpenCV
    # clipLimit defines threshold to limit the contrast in case of noise in our image
    # tileGridSize defines the area size in which local equalization will be performed
    clahe_model = cv2.createCLAHE(clipLimit=1.5, tileGridSize=(32,32))

    # For ease of understanding, we explicitly equalize each channel individually
    colorimage_b = clahe_model.apply(iimg[:,:,0])
    colorimage_g = clahe_model.apply(iimg[:,:,1])
    colorimage_r = clahe_model.apply(iimg[:,:,2])

    # Next we stack our equalized channels back into a single image
    return np.stack((colorimage_b,colorimage_g,colorimage_r), axis=2)

img = image.copy()

img = cv2.medianBlur(img, 3)
img = cv2.detailEnhance(img, sigma_s=300, sigma_r=7)
img = heq(img)
img = bright_contrast(image, -10, -30)  # brightness and contrast

show(image)
show(img)

def draw_col_gist(iimg):
    color = ('b','g','r')
    for i,col in enumerate(color):
        histr, _ = np.histogram(iimg[:,:,i],256,[0,256])
        plt.plot(histr,color = col)
        plt.xlim([0,256])
    plt.show()

draw_col_gist(image)
draw_col_gist(img)

color_ranges = {
    "white": [(0, 0, 90), (255, 130, 255)], # inscrease 130 here for better white detection
    "red": [[(0, 50, 50), (50, 255, 255)], [(140, 50, 50), (200, 255, 255)]],
    "blue": [(85, 102, 40), (194, 255, 255)],
    "black": [(0, 0, 0), (255, 255, 36)],
}

kernel = np.ones((3, 3), np.uint8)  # Generate "drawing" kernel

center_mask = mask_center(img, divk=2)  # Create center mask (half of wd and half of ht), just white rect
center_mask = cv2.cvtColor(center_mask, cv2.COLOR_BGR2GRAY) # normmalize it

mask_black = mask_by_color(img, color_ranges['black'])  # Shades of black
mask_white = mask_by_color(img, color_ranges['white'])  # Shades of white (and invert)

mask_red = mask_by_color(img, color_ranges['red']) # Shades of red and pink
show(mask_red)

# -------- PREPARE RED PLUS FOR DETECTION --------

mask_red = mask_by_color(img, color_ranges['red'])  # Shades of red and pink
mask_red = cv2.subtract(mask_red, mask_black) # Remove black from red (to avoid crooked plus)
mask_red = np.where(center_mask == 0, 0, mask_red) # apply the center mask
mask_red = (cv2.morphologyEx((mask_red), cv2.MORPH_DILATE, kernel, iterations=3))
# This will make the image of the red cross clearer and more integral

# -------- PREPARE RED PLUS FOR DETECTION --------

show(mask_red)

# Init cascade
cascade = cv2.CascadeClassifier()
cascade.load(workpath + "cascade.xml")  # load cascade

# Crop black borders
y_nonzero, x_nonzero = np.nonzero(mask_red)
mask_red_detectable = (mask_red[np.min(y_nonzero):np.max(y_nonzero), np.min(x_nonzero):np.max(x_nonzero)])

# Detect cross using cascade
detected_cross = cascade.detectMultiScale(
    invert(mask_red_detectable), # for some reason, it detects black, not white
    scaleFactor=1.2,
    minNeighbors=3,
    minSize=(1, 1),
    maxSize=(250, 250),
)


print(f"Found {len(detected_cross)} objects using haar cascade!")

haar_rect = {"mindist": 0, "center": (0, 0), "wh": (0, 0)}
detects = []

# Iterate over each of detected rect zones
for detect, (dX, dY, dW, dH) in enumerate(detected_cross):
    plus_center = (dX + dW // 2, dY + dH // 2) # Calculate the center of the rectangle

    if (max(dW, dH) / min(dW, dH)) > 1.35: # Если соотношение сторон совсем-совмем неправильное
        continue

    # Check if new center is closer to the real center than previous one
    distance = sqrt(plus_center[0] ** 2 + plus_center[1] ** 2)

    temp_dict = dict()
    temp_dict["mindist"] = distance
    temp_dict["center"] = plus_center
    temp_dict["wh"] = (dW, dH)
    temp_dict["xy"] = (dX + np.min(x_nonzero), dY + np.min(y_nonzero))

    if distance < haar_rect["mindist"] or haar_rect["mindist"] == 0:
        # Update the haar_rect dictionary with the new closest rectangle's information
        haar_rect.update(temp_dict)
        detects.insert(0, haar_rect) # append to the beginning
    else:
        detects.append(temp_dict) # append to the end

# Draw haar zone rect
for i in detects:
    _ = cv2.rectangle(img,
                (i["xy"][0], i["xy"][1]),
                (i["xy"][0] + i["wh"][0], i["xy"][1] + i["wh"][1]),
                (250, 10, 50),
                2)
show(mask_red)
show(img)

contours, _ = cv2.findContours(mask_red, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
contours = list(contours)  # convert to list
print(f"Found {len(contours)} contours.")

# Сохраним тех. информацию обо всех контурах
cnts_info = []
for i, cnt in enumerate(contours):
    M = cv2.moments(cnt)
    if M["m00"] != 0:
        cX = int(M["m10"] / M["m00"])  # get center X
        cY = int(M["m01"] / M["m00"])  # get center Y
    else:
        continue
    _, (cW, cH), _ = cv2.minAreaRect(cnt) # get width and height
    cv2.drawContours(img,[np.intp(cv2.boxPoints(cv2.minAreaRect(cnt)))],0,(0,0,255),2) # draw red rotated bounding box

    if (max(cW, cH) / min(cW, cH)) > 1.35: # Если соотношение сторон совсем-совмем неправильное
        continue
    cnts_info.append({"w": cW, "h": cH, "x": cX, "y": cY, "i": i})

print(f"Removed {len(contours) - len(cnts_info)} contours.")
pprint(cnts_info)

perfects = []
# Также обхожу каждый элемент (каждую зону)
for detect in detects:
    # Создаём отдельно для каждой зоны диапазоны
    rX = range(
        detect["xy"][0],
        detect["xy"][0] + detect["wh"][0],
    )
    rY = range(
        detect["xy"][1],
        detect["xy"][1] + detect["wh"][1],
    )

    # Обходим каждый контур
    for cnt in cnts_info:
        if cnt['x'] in rX and cnt['y'] in rY:
            print("This's our case!", cnt['i'])
            if len(contours) > 1:
                # Каково расстояние до других контуров (если такие есть)
                flag = False
                for other in [x for i,x in enumerate(cnts_info) if i!=cnt['i']]:
                    dist_to_other = sqrt(abs(cnt['x'] - other['x']) ** 2 + abs(cnt['y'] - other['y']) ** 2)
                    print(dist_to_other / max(cnt['w'], cnt['w']))
                    if dist_to_other / max(cnt['w'], cnt['w']) > 6.5:
                        flag = True
                    else:
                        flag = False
                if flag == True:
                    perfects.append(contours[cnt['i']])

cv2.drawContours(img, perfects, -1, (0, 255, 0), 2)
print(f"Found {len(perfects)} perfect(s)!")

show(img, 600)